<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on CloudKernels</title>
    <link>https://cloudkernels.github.io/posts/</link>
    <description>Recent content in Posts on CloudKernels</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>&amp;copy; CloudKernels 2019-2020</copyright>
    <lastBuildDate>Fri, 09 Jul 2021 10:14:04 +0100</lastBuildDate>
    
	<atom:link href="https://cloudkernels.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Running containers on Firecracker microVMs using kata on kubernetes</title>
      <link>https://cloudkernels.github.io/posts/kata-fc-k3s-k8s/</link>
      <pubDate>Fri, 09 Jul 2021 10:14:04 +0100</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/kata-fc-k3s-k8s/</guid>
      <description>This is the first of a number of posts regarding the orchestration, deployment and scaling of containerized applications in VM sandboxes using kubernetes, kata-containers and AWS Firecracker microVMs. We have gathered some notes during the installation and configuration of the necessary components and we thought they might be useful to the community, especially with regards to the major pain points in trying out recent open-source projects and technologies.
About Orchestration, the Edge, and Kata Containers To manage and orchestrate containers in a cluster, the community is using kubernetes (k8s), a powerful, open-source system for automating the deployment, scaling and management of containerized applications.</description>
    </item>
    
    <item>
      <title>Hardware acceleration in the Age of Functions (vol II)</title>
      <link>https://cloudkernels.github.io/posts/vaccel_v2/</link>
      <pubDate>Fri, 04 Dec 2020 14:14:04 +0100</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/vaccel_v2/</guid>
      <description>In our previous post we spoke about the potential solutions for deploying serverless offerings with hardware acceleration support. With the increasing adoption of the serverless and FaaS paradigms, providers will need to offer some form of hardware acceleration semantics.
For some time now, Amazon has identifed this as a &amp;ldquo;compelling use case&amp;rdquo; for their AWS Firecracker hypervisor which powers the Amazon Lambda service. What is more, they identify traditional techniques for GPU support in VMs such as GPU passthrough comes with limitations and significantly increases the attack surface of the hypervisor.</description>
    </item>
    
    <item>
      <title>Hardware acceleration in the Age of Functions</title>
      <link>https://cloudkernels.github.io/posts/vaccel/</link>
      <pubDate>Mon, 01 Jun 2020 20:29:17 +0000</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/vaccel/</guid>
      <description>The debate on how to deploy applications, monoliths or micro services, is in full swing. Part of this discussion relates to how the new paradigm incorporates support for accessing accelerators, e.g. GPUs, FPGAs. That kind of support has been made available to traditional programming models the last couple of decades and its tooling has evolved to be stable and standardized.
On the other hand, what does it mean for a serverless setup to access an accelerator?</description>
    </item>
    
    <item>
      <title>Fosdem 2020</title>
      <link>https://cloudkernels.github.io/posts/fosdem-2020/</link>
      <pubDate>Wed, 26 Feb 2020 20:10:38 +0000</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/fosdem-2020/</guid>
      <description>Earlier this month we visited FOSDEM, an absolutely open and free event for developers, open-source vendors and enthusiasts to meet, share their ideas and news, and discuss the latest in open source. Talks at FOSDEM are usually organized within several sections: Keynotes, Main tracks, Developer rooms and Lightning talks.
Some people from our team had visited before, but for most of us first timers it was really exciting. Packed keynotes, busy dev rooms, people chatting outside, over coffee, beer, or snacks!</description>
    </item>
    
    <item>
      <title>Build a single-app rootfs for Firecracker MicroVMs</title>
      <link>https://cloudkernels.github.io/posts/fc-rootfs/</link>
      <pubDate>Mon, 21 Oct 2019 10:02:51 +0100</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/fc-rootfs/</guid>
      <description>Spawning applications in the cloud has been made super easy using container frameworks such as docker. For instance running a simple command like the following
docker run --rm -v /path/to/nginx-files:/etc/nginx nginx spawns an NGINX web server, provided you customize config files and the actual HTML files to be served.
This process, inherits NGINX&amp;rsquo;s stock docker hub rootfs, and spawns it as a docker container in a generic Linux container host.</description>
    </item>
    
    <item>
      <title>Porting Firecracker to a Raspberry Pi 4</title>
      <link>https://cloudkernels.github.io/posts/firecracker-rpi4/</link>
      <pubDate>Mon, 21 Oct 2019 09:58:31 +0200</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/firecracker-rpi4/</guid>
      <description>Since we got our hands on the new Raspberry Pi 4, we started exploring how various virtualization technologies behave on the board. First thing we tried is how to run Nabla on it and how it compares to native KVM.
Next thing we wanted to try is firecracker, the notorious micro-VMM that Amazon Lambda &amp;amp; Fargate run on. To our disappointment, firecracker was not yet running on RPi4. So we started looking into coding in the necessary changes :)</description>
    </item>
    
    <item>
      <title>Build a 64bit bootable image for a Raspberry Pi 4</title>
      <link>https://cloudkernels.github.io/posts/rpi4-64bit-image/</link>
      <pubDate>Sun, 14 Jul 2019 00:09:27 +0200</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/rpi4-64bit-image/</guid>
      <description>Given the traction our previous post got, we thought we should jot down the steps to build a 64-bit bootable image for a RPi4. The distro we&amp;rsquo;re most familiar with is Debian, so we&amp;rsquo;ll go with a debian-like distro like Ubuntu. If you don&amp;rsquo;t feel like playing with kernel compilation and FS images, just grab the binary and dd it to an SD card!
First step, download the 64-bit ubuntu server distro for the RPi3:</description>
    </item>
    
    <item>
      <title>Playing with a Raspberry Pi 4 64-bit</title>
      <link>https://cloudkernels.github.io/posts/rpi4-64bit-virt/</link>
      <pubDate>Wed, 10 Jul 2019 14:41:44 +0200</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/rpi4-64bit-virt/</guid>
      <description>Lightweight virtualization is a natural fit for low power devices and, so, seeing that the extremely popular Raspberry Pi line got an upgrade, we were very keen on trying the newly released Raspberry Pi 4 model B.
Getting the board up and running with a 64bit kernel (and a 64bit userland) proved to be kind of a challenge, given that currently there is a number of limitations (SD card not fully working for &amp;gt; 1GB RAM, coherent memory allocations etc.</description>
    </item>
    
    <item>
      <title>How to build a python snippet for running in a Nabla container</title>
      <link>https://cloudkernels.github.io/posts/building-python-snippets-for-nabla/</link>
      <pubDate>Sat, 23 Feb 2019 18:17:51 +0200</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/building-python-snippets-for-nabla/</guid>
      <description>In our previous posts, we saw how to build the toolchain for a Nabla container, and also how we can use this toolchain to run applications as unikernels using Nabla.
In this post, we will be focusing on the steps we need to take into running something actually useful using Nabla. More specifically, we will go through all the steps for building Python3 into a Rumprun unikernel, suitable for running in a Nabla container, and cooking a filesystem that includes a Python script that we wish to run within.</description>
    </item>
    
    <item>
      <title>Build a Nabla Docker Image</title>
      <link>https://cloudkernels.github.io/posts/build-a-nabla-docker-image/</link>
      <pubDate>Sat, 23 Feb 2019 14:41:40 +0200</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/build-a-nabla-docker-image/</guid>
      <description>In this post, we go through the basic steps for containerizing a unikernel application and running it on nabla runnc. Checkout nabla containers and in particular runnc.
How to In order to build a docker image for nabla containers, we have to build:
 the nabla toolstack the unikernel image the docker image  Build the nabla toolstack There&amp;rsquo;s an informative blog post on how to build the nabla rumprun toolstack here.</description>
    </item>
    
    <item>
      <title>Building the Nabla containers toolchain for aarch64</title>
      <link>https://cloudkernels.github.io/posts/building-nabla-aarch64/</link>
      <pubDate>Thu, 24 Jan 2019 16:17:51 +0200</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/building-nabla-aarch64/</guid>
      <description>[UPDATE: Revise instructions and links to use latest upstream nabla repo with merged aarch64 support.] In previous posts, we covered a bit of background on rumprun, the nabla containers fork and our port on aarch64. In this post, we describe how to build everything from source. In order to build a rumprun unikernel for aarch64, the first step is to build the rumprun toolchain.
Clone the relevant repositories:
git clone https://github.</description>
    </item>
    
    <item>
      <title>Run a rumprun unikernel on a RPi3</title>
      <link>https://cloudkernels.github.io/posts/example-rumprun-solo5-on-aarch64/</link>
      <pubDate>Thu, 24 Jan 2019 00:09:27 +0200</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/example-rumprun-solo5-on-aarch64/</guid>
      <description>[UPDATE: Revise instructions to reflect upstream nabla changes.] In this post, we will walk through the steps of compiling, baking, and running an application as a rumprun unikernel on a Rasrberry Pi 3.
In our previous post, we provided some background for Rumprun/rump kernels and Solo5. In short, Rumprun provides the necessary components to run a POSIX compatible application as a unikernel. Solo5 is, essentially, a hardware abstraction layer that provides a very thin interface, or else a minimal attack surface.</description>
    </item>
    
    <item>
      <title>Experiences from porting nabla containers to an ARMv8 board</title>
      <link>https://cloudkernels.github.io/posts/nabla-containers-aarch64/</link>
      <pubDate>Wed, 23 Jan 2019 14:41:44 +0200</pubDate>
      
      <guid>https://cloudkernels.github.io/posts/nabla-containers-aarch64/</guid>
      <description>[UPDATE: Rumprun aarch64 support has now been merged in upstream nabla.] Nabla containers provide a new type of container designed for strong isolation on a host system. The foundation of nabla containers lies in three main components: rumpkernel, solo5, and runnc. The team that built nabla containers extended the rumprun unikernel framework to support solo5 (instead of hardware/baremetal or xen), so that a rumprun-baked unikernel application can be executed on top of a lightweight monitor such as solo5.</description>
    </item>
    
  </channel>
</rss>